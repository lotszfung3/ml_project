{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LSTM_UNITS = 64\n",
    "ITERATIONS = 20000\n",
    "MAX_SEQ_LENGTH = 100\n",
    "WORD_DIM = 100\n",
    "TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the pretrained model:  400000\n",
      "The first word:  the\n",
      "Corresponding vector:  [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "### Load pretrained word2vec model\n",
    "pretrained = pd.read_table('data/glove.6B.100d.txt', header=None, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "word_list = pretrained.loc[:,0].tolist()\n",
    "word_vectors = pretrained.loc[:,1:].values\n",
    "\n",
    "print('Number of words in the pretrained model: ', len(word_list))\n",
    "print('The first word: ', word_list[0])\n",
    "print('Corresponding vector: ', word_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d380meD0W0M</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12T18:01:41.000Z</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>132235</td>\n",
       "      <td>1989</td>\n",
       "      <td>17518</td>\n",
       "      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I know it's been a while since we did this sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  1ZAPwfrtAFY      17.14.11   \n",
       "2  5qpjK5DgCt4      17.14.11   \n",
       "3  puqaWrEC7tY      17.14.11   \n",
       "4  d380meD0W0M      17.14.11   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22  2017-11-13T17:13:01.000Z   \n",
       "1           24  2017-11-13T07:30:00.000Z   \n",
       "2           23  2017-11-12T19:05:24.000Z   \n",
       "3           24  2017-11-13T11:00:04.000Z   \n",
       "4           24  2017-11-12T18:01:41.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3       666           2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "4      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "1              False             False                   False   \n",
       "2              False             False                   False   \n",
       "3              False             False                   False   \n",
       "4              False             False                   False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  \n",
       "2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "3  Today we find out if Link is a Nickelback amat...  \n",
       "4  I know it's been a while since we did this sho...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load the youtube dataset\n",
    "df = pd.read_csv('data/USvideos.csv')\n",
    "\n",
    "df=df[df.dislikes > 0]\n",
    "df=df[df.likes > 0]\n",
    "df[\"description\"] = df[\"description\"].fillna('')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples : 24583\n"
     ]
    }
   ],
   "source": [
    "print('Total number of samples :', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define popularity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"likes_per_views\", \"likes_to_dislikes\", \"log_likes_per_views\", \"log_likes_to_dislikes\", \"diff_per_views\"]\n",
    "\n",
    "df[\"likes_per_views\"]= df[\"likes\"]/df[\"views\"]\n",
    "df[\"likes_to_dislikes\"]=df[\"likes\"]/df[\"dislikes\"]\n",
    "df[\"log_likes_per_views\"]=np.log(df[\"likes\"]/df[\"views\"])\n",
    "df[\"log_likes_to_dislikes\"]=np.log(df[\"likes\"]/df[\"dislikes\"])\n",
    "df[\"diff_per_views\"]=(df[\"likes\"]-df[\"dislikes\"])/df[\"views\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\x00]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" 's\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" 've\", string) \n",
    "    string = re.sub(r\"\\'t\", \" 't\", string) \n",
    "    string = re.sub(r\"\\'re\", \" 're\", string) \n",
    "    string = re.sub(r\"\\'d\", \" 'd\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" 'll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string) \n",
    "    return string.lower()\n",
    "\n",
    "df_c=df[[\"title\",\"channel_title\",\"description\"]]\n",
    "df_c[\"title\"]=df_c[\"title\"].apply(clean_str)\n",
    "df_c[\"channel_title\"]=df_c[\"channel_title\"].apply(clean_str)\n",
    "df_c[\"description\"]=df_c[\"description\"].apply(clean_str)\n",
    "\n",
    "df = df.join(df_c, rsuffix='_clean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the id matrix of descriptions for word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24583, 100)\n",
      "[[182902   1629 123042 ...  10108 194282 180918]\n",
      " [    48     62     49 ...     12     68   3496]\n",
      " [  1716    192    771 ...   1864 336264   7112]\n",
      " ...\n",
      " [     0   3761    213 ...      1    144    929]\n",
      " [    77   6526     13 ...  33161 251142  14035]\n",
      " [ 77523      0     50 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from random import randint\n",
    "\n",
    "if os.path.isfile('data/descriptions_id_matrix.npy'):\n",
    "    # directly load the id matrix\n",
    "    descriptions = np.load('data/descriptions_id_matrix.npy')\n",
    "else:\n",
    "    # map the id of each word in each description, then save to a file\n",
    "    descriptions = []\n",
    "    \n",
    "    for i, string in enumerate(df['description_clean']):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        vec = np.zeros(MAX_SEQ_LENGTH, dtype='int32')\n",
    "        for word_idx, word in enumerate(string.split()):\n",
    "            if word_idx == MAX_SEQ_LENGTH:\n",
    "                break\n",
    "            try:\n",
    "                vec[word_idx] = word_list.index(word)\n",
    "            except ValueError:\n",
    "                # handle unseen word by randomly pick a word to represent it\n",
    "                vec[word_idx] = randint(0,len(word_list)-1)           \n",
    "        descriptions.append(vec)\n",
    "\n",
    "    descriptions = np.array(descriptions)\n",
    "    np.save('data/descriptions_id_matrix', descriptions)\n",
    "\n",
    "print(descriptions.shape)\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform text features by the bag-of-words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<24583x51496 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2225917 stored elements in Compressed Sparse Row format>,\n",
       " <24583x2571 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 47382 stored elements in Compressed Sparse Row format>,\n",
       " <24583x8802 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 186159 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "features = [\"description_clean\", \"channel_title_clean\", \"title_clean\"]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "transformed_features = []\n",
    "for feature in features:\n",
    "    m = vect.fit_transform(df[feature])\n",
    "    m = tfidf.fit_transform(m)\n",
    "    transformed_features.append(m)\n",
    "\n",
    "transformed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting different popularity metrics by description / channel_title / title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   description / channel title / title\n",
      "likes_per_views : -3.0134   0.4704   -1.3667\n",
      "likes_to_dislikes : -0.9927   0.1059   -2.0169\n",
      "log_likes_per_views : -0.4002   0.4581   -0.8905\n",
      "log_likes_to_dislikes : -0.5473   0.2828   -1.3608\n",
      "diff_per_views : -3.0442   0.4614   -1.3247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import linear_model\n",
    "\n",
    "## Compute all scores\n",
    "print(\"                   description / channel title / title\")\n",
    "for metric in metrics:\n",
    "    scores = []\n",
    "    for i in range(len(features)):\n",
    "        cv_score = cross_val_score(linear_model.LinearRegression(), transformed_features[i], df[metric], cv=5) \n",
    "        scores.append(sum(cv_score) / len(cv_score))\n",
    "    print(\"{} : {:.4f}   {:.4f}   {:.4f}\".format(metric, *scores))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting log_likes_per_views by description and channel_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-44.88209404,  -0.45814802,   0.10112746,  -0.05568677,\n",
       "       -35.89735534])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "scores = cross_val_score(linear_model.LinearRegression(),\n",
    "                         sparse.hstack((transformed_features[0], transformed_features[1])),\n",
    "                         df[\"log_likes_per_views\"], cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting different popularity metrics by description / channel_title / title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   description / channel title / title\n",
      "likes_per_views : 0.4201   0.3773   0.2296\n",
      "likes_to_dislikes : 0.1749   0.1781   0.1233\n",
      "log_likes_per_views : 0.4300   0.3668   0.2414\n",
      "log_likes_to_dislikes : 0.3074   0.3097   0.1934\n",
      "diff_per_views : 0.4185   0.3780   0.2292\n"
     ]
    }
   ],
   "source": [
    "print(\"                   description / channel title / title\")\n",
    "for metric in metrics:\n",
    "    scores = []\n",
    "    for i in range(len(features)):\n",
    "        cv_score = cross_val_score(linear_model.SGDRegressor(loss='squared_loss', penalty='l2',\n",
    "                                           alpha=1e-3,learning_rate='optimal',\n",
    "                                           max_iter=1000, tol=None),\n",
    "                                   transformed_features[i], df[metric], cv=5) \n",
    "        scores.append(sum(cv_score) / len(cv_score))\n",
    "    print(\"{} : {:.4f}   {:.4f}   {:.4f}\".format(metric, *scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting log_likes_per_views by description and channel_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50922941, 0.52199136, 0.51840033, 0.41284235, 0.57921615])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(linear_model.SGDRegressor(loss='squared_loss', penalty='l2',\n",
    "                                           alpha=1e-3,learning_rate='optimal',\n",
    "                                           max_iter=1000, tol=None),\n",
    "                         sparse.hstack((transformed_features[0], transformed_features[1])),\n",
    "                         df[\"log_likes_per_views\"], cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting diff_per_views  by description and channel_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49068005, 0.47777817, 0.51269925, 0.50429451, 0.50616807])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(linear_model.SGDRegressor(loss='squared_loss', penalty='l2',\n",
    "                                           alpha=1e-3,learning_rate='optimal',\n",
    "                                           max_iter=1000, tol=None),\n",
    "                         sparse.hstack((transformed_features[0], transformed_features[1])),\n",
    "                         df[\"diff_per_views\"], cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting log_likes_per_views by all three features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49201516, 0.49087779, 0.53277965, 0.53168177, 0.51318397])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(linear_model.SGDRegressor(loss='squared_loss', penalty='l2',\n",
    "                                           alpha=1e-3,learning_rate='optimal',\n",
    "                                           max_iter=1000, tol=None),\n",
    "                         sparse.hstack((transformed_features[0],transformed_features[1],transformed_features[2])),\n",
    "                         df[\"diff_per_views\"], cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting different popularity metrics by description / channel_title / title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   description / channel title / title\n",
      "likes_per_views : 0.1683   0.0721   0.0434\n",
      "likes_to_dislikes : 0.0363   0.0316   0.0300\n",
      "log_likes_per_views : 0.1751   0.0592   0.0375\n",
      "log_likes_to_dislikes : 0.1158   0.0697   0.0333\n",
      "diff_per_views : 0.1650   0.0753   0.0448\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "print(\"                   description / channel title / title\")\n",
    "for metric in metrics:\n",
    "    scores = []\n",
    "    for i in range(len(features)):\n",
    "        cv_score = cross_val_score(ensemble.RandomForestRegressor(n_estimators=20, max_depth=10, max_features='sqrt'),\n",
    "                                   transformed_features[i], df[metric], cv=5) \n",
    "        scores.append(sum(cv_score) / len(cv_score))\n",
    "    print(\"{} : {:.4f}   {:.4f}   {:.4f}\".format(metric, *scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting different popularity metrics by description / channel_title / title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   description / channel title / title\n",
      "likes_per_views : -1.2004   -0.4160   -0.8005\n",
      "likes_to_dislikes : -1.6898   -0.7399   -1.4303\n",
      "log_likes_per_views : 0.0509   -0.1751   -0.3033\n",
      "log_likes_to_dislikes : 0.0714   -0.0764   -0.1064\n",
      "diff_per_views : -0.5223   -0.3595   -0.6445\n"
     ]
    }
   ],
   "source": [
    "print(\"                   description / channel title / title\")\n",
    "for metric in metrics:\n",
    "    scores = []\n",
    "    for i in range(len(features)):\n",
    "        cv_score = cross_val_score(ensemble.AdaBoostRegressor(), transformed_features[i], df[metric], cv=5)\n",
    "        scores.append(sum(cv_score) / len(cv_score))\n",
    "    print(\"{} : {:.4f}   {:.4f}   {:.4f}\".format(metric, *scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting different popularity metrics by description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "### Generate batch, randomly pick some rows in X, y   \n",
    "def get_batch(X, y):\n",
    "    n = 1 if len(y.shape) == 1 else y.shape[1]\n",
    "    labels = np.zeros([BATCH_SIZE, n])\n",
    "    arr = np.zeros([BATCH_SIZE, MAX_SEQ_LENGTH])\n",
    "    for i in range(BATCH_SIZE):\n",
    "        num = randint(0, len(y)-1) \n",
    "        arr[i], labels[i] = X[num], y.iloc[num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- likes_per_views -----------------\n",
      "INFO:tensorflow:Restoring parameters from models/model-likes_per_views\\pretrained_lstm.ckpt-20000\n",
      "Average r^2 score for testing = 0.7540 \n",
      "\n",
      "-------------- likes_to_dislikes -----------------\n",
      "INFO:tensorflow:Restoring parameters from models/model-likes_to_dislikes\\pretrained_lstm.ckpt-20000\n",
      "Average r^2 score for testing = 0.8381 \n",
      "\n",
      "-------------- log_likes_per_views -----------------\n",
      "INFO:tensorflow:Restoring parameters from models/model-log_likes_per_views\\pretrained_lstm.ckpt-20000\n",
      "Average r^2 score for testing = 0.8870 \n",
      "\n",
      "-------------- log_likes_to_dislikes -----------------\n",
      "INFO:tensorflow:Restoring parameters from models/model-log_likes_to_dislikes\\pretrained_lstm.ckpt-20000\n",
      "Average r^2 score for testing = 0.8923 \n",
      "\n",
      "-------------- diff_per_views -----------------\n",
      "INFO:tensorflow:Restoring parameters from models/model-diff_per_views\\pretrained_lstm.ckpt-20000\n",
      "Average r^2 score for testing = 0.7588 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, ytrain, ytest = train_test_split(descriptions, df[metrics], test_size=0.2, random_state=42)\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    print (\"-------------- %s -----------------\" % metric)\n",
    "    y_train = ytrain[metric]\n",
    "    y_test = ytest[metric]\n",
    "    \n",
    "    \n",
    "    ############# Define the model ################\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # define placeholder\n",
    "    y = tf.placeholder(tf.float32, [BATCH_SIZE, NUM_CLASSES])\n",
    "    input_X = tf.placeholder(tf.int32, [BATCH_SIZE, MAX_SEQ_LENGTH])\n",
    "\n",
    "    # look up word vectors for the ids in each batch of data\n",
    "    data = tf.Variable(tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH, WORD_DIM]),dtype=tf.float32)\n",
    "    data = tf.nn.embedding_lookup(word_vectors,input_X)\n",
    "    data = tf.cast(data,tf.float32)\n",
    "\n",
    "    # lstm cell\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(LSTM_UNITS)\n",
    "    lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    output, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "    weight = tf.Variable(tf.random_uniform([LSTM_UNITS, NUM_CLASSES]))\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]))\n",
    "    output = tf.transpose(output, [1, 0, 2])\n",
    "    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "    y_pred = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "    # sum of squared error loss\n",
    "    loss = tf.reduce_mean(tf.square(y_pred-y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    # Coefficient of determination scores\n",
    "    residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "    total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "    r2 = tf.subtract(1.0, tf.div(residual, total))\n",
    "\n",
    "\n",
    "    ############ TRAINING ############\n",
    "\n",
    "    if os.path.isdir(\"models/model-%s\" % metric):\n",
    "        sess = tf.InteractiveSession()\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(\"models/model-%s\" % metric))\n",
    "    else:\n",
    "        sess = tf.InteractiveSession()\n",
    "        saver = tf.train.Saver()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if TRAINING:\n",
    "        tf.summary.scalar('Loss', loss)\n",
    "        tf.summary.scalar('Score', r2)\n",
    "        merged = tf.summary.merge_all()\n",
    "        logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "        for i in range(1,ITERATIONS+1):\n",
    "            nextBatch, nextBatchY = get_batch(X_train, y_train)\n",
    "            sess.run(optimizer, {input_X: nextBatch, y: nextBatchY})\n",
    "\n",
    "            # Write summary to Tensorboard\n",
    "            if i % 50 == 0:\n",
    "                summary = sess.run(merged, {input_X: nextBatch, y : nextBatchY})\n",
    "                writer.add_summary(summary, i)\n",
    "\n",
    "            #Save the model every 10,000 training iterations\n",
    "            if i % 10000 == 0:\n",
    "                save_path = saver.save(sess, \"models/model-%s/pretrained_lstm.ckpt\" % metric, global_step=i)\n",
    "                print(\"saved to %s\" % save_path)\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "    ############ TESTING ############    \n",
    "    scores=[]\n",
    "    for i in range(3000):\n",
    "        nextBatchX, nextBatchY = get_batch(X_test, y_test);\n",
    "        scores.append((sess.run(r2, {input_X: nextBatchX, y: nextBatchY})))\n",
    "        #print(\"Score for batch %d : %d\" % (i, score[i]))\n",
    "    print(\"Average r^2 score for testing = %.4f \\n\" % (sum(scores)/len(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting log_likes_per_views by channel title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 3\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "# Build the id matrix for channel titles\n",
    "titles = []\n",
    "for i, string in enumerate(df[\"channel_title_clean\"]):\n",
    "    vec = np.zeros(MAX_SEQ_LENGTH, dtype='int32')\n",
    "    for word_idx, word in enumerate(string.split()):\n",
    "        if word_idx == MAX_SEQ_LENGTH:\n",
    "            break\n",
    "        try:\n",
    "            vec[word_idx] = word_list.index(word)\n",
    "        except ValueError:\n",
    "            # handle unseen word by randomly pick a word to represent it\n",
    "            vec[word_idx] = randint(0,len(word_list)-1)         \n",
    "    titles.append(vec)\n",
    "\n",
    "# split train and test set\n",
    "titles = np.array(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model-channel_title\\pretrained_lstm.ckpt-100000\n",
      "Average score = 0.4687 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(titles, df[\"log_likes_per_views\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "############# Define the model ################\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define placeholder\n",
    "y = tf.placeholder(tf.float32, [BATCH_SIZE, NUM_CLASSES])\n",
    "input_X = tf.placeholder(tf.int32, [BATCH_SIZE, MAX_SEQ_LENGTH])\n",
    "\n",
    "# look up word vectors for the ids in each batch of data\n",
    "data = tf.Variable(tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH, WORD_DIM]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(word_vectors,input_X)\n",
    "data = tf.cast(data,tf.float32)\n",
    "\n",
    "# lstm cell\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(LSTM_UNITS)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "output, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_uniform([LSTM_UNITS, NUM_CLASSES]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]))\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "y_pred = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "# sum of squared error loss\n",
    "loss = tf.reduce_mean(tf.square(y_pred-y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Coefficient of determination scores\n",
    "residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "r2 = tf.subtract(1.0, tf.div(residual, total))\n",
    "\n",
    "\n",
    "############ TRAINING ############\n",
    "\n",
    "if os.path.isdir(\"models/model-channel_title\"):\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"models/model-channel_title\"))\n",
    "else:\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if TRAINING:\n",
    "    tf.summary.scalar('Loss', loss)\n",
    "    tf.summary.scalar('Score', r2)\n",
    "    merged = tf.summary.merge_all()\n",
    "    logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "    for i in range(1,100001):\n",
    "        nextBatch, nextBatchY = get_batch(X_train, y_train)\n",
    "        sess.run(optimizer, {input_X: nextBatch, y: nextBatchY})\n",
    "\n",
    "        # Write summary to Tensorboard\n",
    "        if i % 50 == 0:\n",
    "            summary = sess.run(merged, {input_X: nextBatch, y : nextBatchY})\n",
    "            writer.add_summary(summary, i)\n",
    "\n",
    "        #Save the model every 10,000 training iterations\n",
    "        if i % 10000 == 0:\n",
    "            save_path = saver.save(sess, \"models/model-channel_title/pretrained_lstm.ckpt\", global_step=i)\n",
    "            print(\"saved to %s\" % save_path)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "############ TESTING ############    \n",
    "scores=[]\n",
    "for i in range(3000):\n",
    "    nextBatchX, nextBatchY = get_batch(X_test, y_test);\n",
    "    scores.append((sess.run(r2, {input_X: nextBatchX, y: nextBatchY})))\n",
    "    #print(\"Score for batch %d : %d\" % (i, score[i]))\n",
    "print(\"Average score = %.4f \\n\" % (sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting log_likes_to_dislikes by channel title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-10000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-20000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-30000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-40000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-50000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-60000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-70000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-80000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-90000\n",
      "saved to models/model-channel_title1/pretrained_lstm.ckpt-100000\n",
      "Average score = 0.3901 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(titles, df[\"log_likes_to_dislikes\"], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "############# Define the model ################\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define placeholder\n",
    "y = tf.placeholder(tf.float32, [BATCH_SIZE, NUM_CLASSES])\n",
    "input_X = tf.placeholder(tf.int32, [BATCH_SIZE, MAX_SEQ_LENGTH])\n",
    "\n",
    "# look up word vectors for the ids in each batch of data\n",
    "data = tf.Variable(tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH, WORD_DIM]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(word_vectors,input_X)\n",
    "data = tf.cast(data,tf.float32)\n",
    "\n",
    "# lstm cell\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(LSTM_UNITS)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "output, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.random_uniform([LSTM_UNITS, NUM_CLASSES]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]))\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "y_pred = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "# sum of squared error loss\n",
    "loss = tf.reduce_mean(tf.square(y_pred-y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "# Coefficient of determination scores\n",
    "residual = tf.reduce_sum(tf.square(tf.subtract(y, y_pred)))\n",
    "total = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y))))\n",
    "r2 = tf.subtract(1.0, tf.div(residual, total))\n",
    "\n",
    "\n",
    "############ TRAINING ############\n",
    "\n",
    "if os.path.isdir(\"models/model-channel_title1\"):\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(\"models/model-channel_title1\"))\n",
    "else:\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if TRAINING:\n",
    "    tf.summary.scalar('Loss', loss)\n",
    "    tf.summary.scalar('Score', r2)\n",
    "    merged = tf.summary.merge_all()\n",
    "    logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "\n",
    "    for i in range(1,100001):\n",
    "        nextBatch, nextBatchY = get_batch(X_train, y_train)\n",
    "        sess.run(optimizer, {input_X: nextBatch, y: nextBatchY})\n",
    "\n",
    "        # Write summary to Tensorboard\n",
    "        if i % 50 == 0:\n",
    "            summary = sess.run(merged, {input_X: nextBatch, y : nextBatchY})\n",
    "            writer.add_summary(summary, i)\n",
    "\n",
    "        #Save the model every 10,000 training iterations\n",
    "        if i % 10000 == 0:\n",
    "            save_path = saver.save(sess, \"models/model-channel_title1/pretrained_lstm.ckpt\", global_step=i)\n",
    "            print(\"saved to %s\" % save_path)\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "############ TESTING ############    \n",
    "scores=[]\n",
    "for i in range(3000):\n",
    "    nextBatchX, nextBatchY = get_batch(X_test, y_test);\n",
    "    scores.append((sess.run(r2, {input_X: nextBatchX, y: nextBatchY})))\n",
    "    #print(\"Score for batch %d : %d\" % (i, score[i]))\n",
    "print(\"Average score = %.4f \\n\" % (sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
